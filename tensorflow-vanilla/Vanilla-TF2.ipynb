{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA0FvFe966r9"
   },
   "source": [
    "## Red neuronal Perceptrón multicapa  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "babYmP-H61wB",
    "outputId": "78902515-ad4c-4f1d-ecdd-c56f19b21ec1"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nP5zZjkD7sWw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "cl5FegN27wzO",
    "outputId": "2bf05a13-20d6-40f9-a9ec-9c80b8b6af49"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "fashion_mnist = load_data()\n",
    "#read_data_sets(\"exercice/data\", one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq-Bj_Kar14P"
   },
   "source": [
    "## Separando el conjunto de datos en Entrenamiento y prueba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Pjokko6Mr14Q"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)=fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2_1FAsMYr14W",
    "outputId": "36c1aa57-b8dd-43b8-9e16-1ab91b45b474"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pwUO3WIUr14b"
   },
   "outputs": [],
   "source": [
    "imagendemo=x_train[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OkOVIgvnr14g",
    "outputId": "8c23d027-0f73-4158-acd9-1eec4a244be3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21adbcf0c70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(imagendemo,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwRo-mx_r14l"
   },
   "source": [
    "Las etiquetas numéricas pueden ser transformadas al nombre de la clase correspondiente usando el siguiente diccionario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ypNFROm8r14m"
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    " 0: \"T-shirt/top\",\n",
    " 1: \"Trouser\",\n",
    " 2: \"Pullover\",\n",
    " 3: \"Dress\",\n",
    " 4: \"Coat\",\n",
    " 5: \"Sandal\",\n",
    " 6: \"Shirt\",\n",
    " 7: \"Sneaker\",\n",
    " 8: \"Bag\",\n",
    " 9: \"Ankle boot\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "edjTF1COr14q",
    "outputId": "6e5ff608-42a9-4fcd-bd49-44382dbb70e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssgZkgSJ2HIX",
    "outputId": "298945c8-ddc6-4138-eaa0-f6318ccc249c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=x_train.reshape(-1,28*28).astype('float32')/255\n",
    "x_test=x_test.reshape(-1,28*28).astype('float32')/255\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sniyszYT6wjg",
    "outputId": "1d468912-63bc-442c-926e-dd0c2161b535"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIuVO17v31zI"
   },
   "source": [
    "Las salidas y deben se codificadas en one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJ-cam6z4I-t",
    "outputId": "6a220fb7-c0fc-43a0-9a27-b497787ea881"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# onehot encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_train_onehot = onehot_encoder.fit_transform(y_train)\n",
    "\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "y_test_onehot = onehot_encoder.fit_transform(y_test)\n",
    "\n",
    "y_train_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUPvkJ1URpmy"
   },
   "source": [
    "### Declarando la arquitectura\n",
    "\n",
    "Generando función "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8Y2usE7in_2p"
   },
   "outputs": [],
   "source": [
    "class DNN_model(object):\n",
    "    def __init__(self,\n",
    "                 n_nodes_hl1=500,\n",
    "                 n_nodes_hl2=500,\n",
    "                 n_nodes_hl3=500,\n",
    "                 n_classes=10):\n",
    "        self.h1LW = tf.Variable(np.random.randn(784, n_nodes_hl1),name=\"hl1weigths\",dtype=\"float32\")\n",
    "        self.h1LB = tf.Variable(np.random.randn(n_nodes_hl1),name=\"hl1bias\",dtype=\"float32\")\n",
    "        self.h2LW = tf.Variable(np.random.randn(n_nodes_hl1, n_nodes_hl2),name=\"hl2weigths\",dtype=\"float32\")\n",
    "        self.h2LB = tf.Variable(np.random.randn(n_nodes_hl2),name=\"hl2bias\",dtype=\"float32\")\n",
    "        self.h3LW = tf.Variable(np.random.randn(n_nodes_hl2, n_nodes_hl3),name=\"hl3weigths\",dtype=\"float32\")\n",
    "        self.h3LB = tf.Variable(np.random.randn(n_nodes_hl3),name=\"hl3bias\",dtype=\"float32\")\n",
    "        self.outW = tf.Variable(np.random.randn(n_nodes_hl3, n_classes),name=\"outweigths\",dtype=\"float32\")\n",
    "        self.outB = tf.Variable(np.random.randn(n_classes),name=\"outbias\",dtype=\"float32\")\n",
    "        self.trainable_variables =[self.h1LW,self.h1LB,self.h2LW,self.h2LB,self.h3LW,self.h3LB,self.outW,self.outB]          \n",
    "    def __call__(self,x): \n",
    "        # Declarando la arquitectura\n",
    "\n",
    "        l1 = tf.add(tf.matmul(x,self.h1LW), self.h1LB)\n",
    "        l1 = tf.nn.relu(l1)\n",
    "\n",
    "        l2 = tf.add(tf.matmul(l1,self.h2LW), self.h2LB)\n",
    "        l2 = tf.nn.relu(l2)\n",
    "\n",
    "        l3 = tf.add(tf.matmul(l2,self.h3LW), self.h3LB)\n",
    "        l3 = tf.nn.relu(l3)\n",
    "\n",
    "        output = tf.add(tf.matmul(l3,self.outW),self.outB)\n",
    "        return output\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SB_ocuTDRpm2",
    "outputId": "99e4c9d8-c2f9-4945-cdef-38d130db6a32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 10), dtype=float32, numpy=\n",
       "array([[ -52733.266  ,    7753.1616 ,  -10880.492  ,  115901.07   ,\n",
       "         -70552.94   ,   97217.03   ,  -70099.57   ,   22462.123  ,\n",
       "          76632.45   ,  -16206.486  ],\n",
       "       [ -22163.848  ,     363.61456,  -25753.879  ,   44497.457  ,\n",
       "         -52355.047  ,   48610.734  ,  -56358.543  ,   -6185.933  ,\n",
       "          79992.234  ,   10074.744  ],\n",
       "       [ -29290.379  ,   76153.055  ,  -33143.81   ,   71710.41   ,\n",
       "         -75714.28   ,   88279.3    , -137203.1    ,   35346.832  ,\n",
       "          52936.62   ,   37374.516  ],\n",
       "       [ -44881.977  ,   31174.078  ,  -21571.053  ,  177100.78   ,\n",
       "         -91072.02   ,   92110.32   , -112662.23   ,   56604.438  ,\n",
       "          69777.7    ,   47955.637  ],\n",
       "       [ -40422.453  ,    8489.564  ,  -21179.104  ,  114281.55   ,\n",
       "         -58269.395  ,   49327.65   ,  -57400.695  ,   11123.976  ,\n",
       "          32102.422  ,    2487.6458 ],\n",
       "       [ -45963.895  ,   11685.69   ,  -22049.475  ,  107690.484  ,\n",
       "         -52208.51   ,   85248.695  , -101540.2    ,   32339.346  ,\n",
       "          99116.27   ,   31500.059  ]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNN = DNN_model()\n",
    "DNN(x_train[24:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOibydeqEJem"
   },
   "source": [
    "Seleccionar un optimizador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "A1on8R5B5sLw"
   },
   "outputs": [],
   "source": [
    "#optimizador = tf.keras.optimizers.Adam(learning_rate=0.001 )\n",
    "optimizador = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTAlRDeyAo_7"
   },
   "source": [
    "### Definir las metricas a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GrMfhkpuAug4"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnND6eokQQyb"
   },
   "source": [
    "### Calculo de gradientes y ajuste "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BTTZsKgM_5U3"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model,tdata, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tdata)\n",
    "        #calculo de una funcion de error \n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( labels, predictions ))\n",
    "   \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    capped_grads_and_vars = [(grad,model.trainable_variables[index]) for index, grad in enumerate(gradients)]\n",
    "    optimizador.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "97g8_iaQPtUk"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(model,tdata, labels):\n",
    "    predictions = model(tdata)\n",
    "    t_loss =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels, predictions))\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class batch_loader:\n",
    "    def __init__(self,data_x,data_y,batch_size=100):\n",
    "        self.x=data_x\n",
    "        self.y=data_y\n",
    "        self.index=0\n",
    "        self.len=len(data_x)\n",
    "        self.batch_size=batch_size\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index+self.batch_size<=self.len:\n",
    "            to_return=(self.x[self.index:self.index+self.batch_size,:],self.y[self.index:self.index+self.batch_size,:])\n",
    "            self.index+=self.batch_size\n",
    "            return to_return\n",
    "        elif self.index<self.batch_size:\n",
    "            to_return=(self.x[self.index:,:],self.y[self.index:,:])\n",
    "            self.index=self.len\n",
    "            return to_return\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXi8dwZEN4fe"
   },
   "source": [
    "## función de entrenamiento  y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BKZfZ5G4BKlp"
   },
   "outputs": [],
   "source": [
    "\n",
    "def fitting(model,train_x,train_y,test_x,test_y,EPOCHS,batch_size=100):\n",
    "    for epoch in range(EPOCHS):\n",
    "        i=0\n",
    "        for batch_x, batch_y in batch_loader(train_x,train_y,batch_size):\n",
    "              train_step(model,batch_x,batch_y)\n",
    "\n",
    "        test_step(model,test_x,test_y)\n",
    "\n",
    "        template = 'Epoch {:02}, Perdida: {:.2f}, Exactitud: {:.2f}, Perdida de prueba: {:.2f}, Exactitud de prueba: {:.2f}'\n",
    "        print(template.format(epoch+1,\n",
    "                             train_loss.result(),\n",
    "                            train_accuracy.result()*100,\n",
    "                            test_loss.result(),\n",
    "                            test_accuracy.result()*100))\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "QFKfo19DRpnB",
    "outputId": "3ed5a542-933d-4c94-8635-8a5c5693e136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Perdida: 3808.97, Exactitud: 73.91, Perdida de prueba: 1955.91, Exactitud de prueba: 78.89\n",
      "Epoch 02, Perdida: 1407.21, Exactitud: 80.96, Perdida de prueba: 1420.51, Exactitud de prueba: 80.76\n",
      "Epoch 03, Perdida: 963.76, Exactitud: 83.07, Perdida de prueba: 1097.08, Exactitud de prueba: 82.41\n",
      "Epoch 04, Perdida: 716.75, Exactitud: 84.50, Perdida de prueba: 952.80, Exactitud de prueba: 82.54\n",
      "Epoch 05, Perdida: 570.51, Exactitud: 85.46, Perdida de prueba: 844.69, Exactitud de prueba: 83.46\n",
      "Epoch 06, Perdida: 455.72, Exactitud: 86.64, Perdida de prueba: 823.83, Exactitud de prueba: 82.69\n",
      "Epoch 07, Perdida: 393.64, Exactitud: 87.24, Perdida de prueba: 740.26, Exactitud de prueba: 83.79\n",
      "Epoch 08, Perdida: 322.51, Exactitud: 88.19, Perdida de prueba: 724.75, Exactitud de prueba: 82.68\n",
      "Epoch 09, Perdida: 277.75, Exactitud: 88.74, Perdida de prueba: 771.78, Exactitud de prueba: 82.61\n",
      "Epoch 10, Perdida: 259.05, Exactitud: 89.06, Perdida de prueba: 671.53, Exactitud de prueba: 83.33\n",
      "Epoch 11, Perdida: 219.16, Exactitud: 89.79, Perdida de prueba: 661.07, Exactitud de prueba: 83.55\n",
      "Epoch 12, Perdida: 194.61, Exactitud: 90.29, Perdida de prueba: 640.40, Exactitud de prueba: 82.72\n",
      "Epoch 13, Perdida: 175.37, Exactitud: 90.70, Perdida de prueba: 632.17, Exactitud de prueba: 83.25\n",
      "Epoch 14, Perdida: 154.08, Exactitud: 91.24, Perdida de prueba: 602.56, Exactitud de prueba: 83.50\n",
      "Epoch 15, Perdida: 140.77, Exactitud: 91.61, Perdida de prueba: 587.36, Exactitud de prueba: 83.57\n",
      "Epoch 16, Perdida: 126.48, Exactitud: 92.13, Perdida de prueba: 563.63, Exactitud de prueba: 84.23\n",
      "Epoch 17, Perdida: 113.38, Exactitud: 92.49, Perdida de prueba: 550.71, Exactitud de prueba: 83.93\n",
      "Epoch 18, Perdida: 109.85, Exactitud: 92.57, Perdida de prueba: 583.15, Exactitud de prueba: 83.69\n",
      "Epoch 19, Perdida: 102.50, Exactitud: 92.82, Perdida de prueba: 559.09, Exactitud de prueba: 84.37\n",
      "Epoch 20, Perdida: 93.18, Exactitud: 93.21, Perdida de prueba: 506.26, Exactitud de prueba: 84.21\n",
      "Epoch 21, Perdida: 89.14, Exactitud: 93.38, Perdida de prueba: 585.10, Exactitud de prueba: 83.34\n",
      "Epoch 22, Perdida: 81.40, Exactitud: 93.59, Perdida de prueba: 523.54, Exactitud de prueba: 84.42\n",
      "Epoch 23, Perdida: 72.45, Exactitud: 93.95, Perdida de prueba: 525.66, Exactitud de prueba: 84.57\n",
      "Epoch 24, Perdida: 72.30, Exactitud: 94.01, Perdida de prueba: 516.74, Exactitud de prueba: 84.85\n",
      "Epoch 25, Perdida: 66.65, Exactitud: 94.27, Perdida de prueba: 542.34, Exactitud de prueba: 84.00\n",
      "Epoch 26, Perdida: 63.32, Exactitud: 94.46, Perdida de prueba: 493.08, Exactitud de prueba: 84.35\n",
      "Epoch 27, Perdida: 57.85, Exactitud: 94.74, Perdida de prueba: 499.16, Exactitud de prueba: 84.94\n",
      "Epoch 28, Perdida: 55.77, Exactitud: 94.84, Perdida de prueba: 476.34, Exactitud de prueba: 85.03\n",
      "Epoch 29, Perdida: 50.40, Exactitud: 95.10, Perdida de prueba: 465.48, Exactitud de prueba: 84.91\n",
      "Epoch 30, Perdida: 52.48, Exactitud: 95.20, Perdida de prueba: 503.36, Exactitud de prueba: 85.22\n",
      "Epoch 31, Perdida: 45.66, Exactitud: 95.38, Perdida de prueba: 469.94, Exactitud de prueba: 84.72\n",
      "Epoch 32, Perdida: 45.85, Exactitud: 95.50, Perdida de prueba: 483.24, Exactitud de prueba: 84.55\n",
      "Epoch 33, Perdida: 42.09, Exactitud: 95.66, Perdida de prueba: 474.52, Exactitud de prueba: 84.16\n",
      "Epoch 34, Perdida: 44.68, Exactitud: 95.57, Perdida de prueba: 458.59, Exactitud de prueba: 85.02\n",
      "Epoch 35, Perdida: 41.67, Exactitud: 95.77, Perdida de prueba: 462.89, Exactitud de prueba: 85.16\n",
      "Epoch 36, Perdida: 36.93, Exactitud: 96.06, Perdida de prueba: 456.94, Exactitud de prueba: 85.24\n",
      "Epoch 37, Perdida: 37.82, Exactitud: 96.01, Perdida de prueba: 449.11, Exactitud de prueba: 85.28\n",
      "Epoch 38, Perdida: 36.08, Exactitud: 96.11, Perdida de prueba: 462.90, Exactitud de prueba: 85.31\n",
      "Epoch 39, Perdida: 38.25, Exactitud: 95.98, Perdida de prueba: 459.91, Exactitud de prueba: 85.56\n",
      "Epoch 40, Perdida: 35.98, Exactitud: 96.21, Perdida de prueba: 434.73, Exactitud de prueba: 85.80\n",
      "Epoch 41, Perdida: 32.92, Exactitud: 96.33, Perdida de prueba: 424.72, Exactitud de prueba: 85.84\n",
      "Epoch 42, Perdida: 32.73, Exactitud: 96.37, Perdida de prueba: 444.53, Exactitud de prueba: 85.80\n",
      "Epoch 43, Perdida: 30.34, Exactitud: 96.58, Perdida de prueba: 464.10, Exactitud de prueba: 85.35\n",
      "Epoch 44, Perdida: 27.04, Exactitud: 96.81, Perdida de prueba: 451.31, Exactitud de prueba: 85.71\n",
      "Epoch 45, Perdida: 29.01, Exactitud: 96.62, Perdida de prueba: 437.71, Exactitud de prueba: 85.37\n",
      "Epoch 46, Perdida: 29.69, Exactitud: 96.59, Perdida de prueba: 418.66, Exactitud de prueba: 86.08\n",
      "Epoch 47, Perdida: 28.12, Exactitud: 96.82, Perdida de prueba: 436.09, Exactitud de prueba: 85.61\n",
      "Epoch 48, Perdida: 28.02, Exactitud: 96.79, Perdida de prueba: 419.45, Exactitud de prueba: 85.90\n",
      "Epoch 49, Perdida: 27.41, Exactitud: 96.79, Perdida de prueba: 432.16, Exactitud de prueba: 85.52\n",
      "Epoch 50, Perdida: 27.90, Exactitud: 96.78, Perdida de prueba: 433.41, Exactitud de prueba: 86.30\n",
      "Epoch 51, Perdida: 25.56, Exactitud: 97.02, Perdida de prueba: 423.96, Exactitud de prueba: 85.99\n",
      "Epoch 52, Perdida: 25.87, Exactitud: 96.98, Perdida de prueba: 438.41, Exactitud de prueba: 85.99\n",
      "Epoch 53, Perdida: 25.69, Exactitud: 96.99, Perdida de prueba: 440.96, Exactitud de prueba: 86.06\n",
      "Epoch 54, Perdida: 24.88, Exactitud: 97.04, Perdida de prueba: 451.94, Exactitud de prueba: 85.44\n",
      "Epoch 55, Perdida: 22.56, Exactitud: 97.27, Perdida de prueba: 444.08, Exactitud de prueba: 85.61\n",
      "Epoch 56, Perdida: 22.52, Exactitud: 97.29, Perdida de prueba: 432.67, Exactitud de prueba: 86.16\n",
      "Epoch 57, Perdida: 23.48, Exactitud: 97.18, Perdida de prueba: 428.54, Exactitud de prueba: 85.79\n",
      "Epoch 58, Perdida: 22.82, Exactitud: 97.23, Perdida de prueba: 435.18, Exactitud de prueba: 86.12\n",
      "Epoch 59, Perdida: 24.50, Exactitud: 97.17, Perdida de prueba: 422.83, Exactitud de prueba: 86.01\n",
      "Epoch 60, Perdida: 20.24, Exactitud: 97.45, Perdida de prueba: 432.39, Exactitud de prueba: 85.93\n",
      "Epoch 61, Perdida: 20.25, Exactitud: 97.51, Perdida de prueba: 414.22, Exactitud de prueba: 86.20\n",
      "Epoch 62, Perdida: 18.84, Exactitud: 97.61, Perdida de prueba: 399.36, Exactitud de prueba: 86.60\n",
      "Epoch 63, Perdida: 19.81, Exactitud: 97.53, Perdida de prueba: 429.89, Exactitud de prueba: 86.24\n",
      "Epoch 64, Perdida: 21.70, Exactitud: 97.38, Perdida de prueba: 435.86, Exactitud de prueba: 85.81\n",
      "Epoch 65, Perdida: 17.52, Exactitud: 97.79, Perdida de prueba: 476.06, Exactitud de prueba: 85.89\n",
      "Epoch 66, Perdida: 17.52, Exactitud: 97.78, Perdida de prueba: 407.10, Exactitud de prueba: 87.02\n",
      "Epoch 67, Perdida: 17.98, Exactitud: 97.65, Perdida de prueba: 426.83, Exactitud de prueba: 86.29\n",
      "Epoch 68, Perdida: 17.22, Exactitud: 97.76, Perdida de prueba: 438.68, Exactitud de prueba: 86.30\n",
      "Epoch 69, Perdida: 17.84, Exactitud: 97.68, Perdida de prueba: 440.08, Exactitud de prueba: 86.41\n",
      "Epoch 70, Perdida: 19.76, Exactitud: 97.68, Perdida de prueba: 440.70, Exactitud de prueba: 86.59\n",
      "Epoch 71, Perdida: 16.16, Exactitud: 97.88, Perdida de prueba: 443.89, Exactitud de prueba: 86.51\n",
      "Epoch 72, Perdida: 14.81, Exactitud: 98.11, Perdida de prueba: 402.43, Exactitud de prueba: 86.79\n",
      "Epoch 73, Perdida: 16.29, Exactitud: 97.92, Perdida de prueba: 418.43, Exactitud de prueba: 86.61\n",
      "Epoch 74, Perdida: 16.00, Exactitud: 98.01, Perdida de prueba: 423.51, Exactitud de prueba: 86.27\n",
      "Epoch 75, Perdida: 16.08, Exactitud: 98.03, Perdida de prueba: 397.84, Exactitud de prueba: 86.63\n",
      "Epoch 76, Perdida: 16.29, Exactitud: 97.95, Perdida de prueba: 413.59, Exactitud de prueba: 86.57\n",
      "Epoch 77, Perdida: 17.53, Exactitud: 97.82, Perdida de prueba: 424.85, Exactitud de prueba: 86.80\n",
      "Epoch 78, Perdida: 14.85, Exactitud: 98.13, Perdida de prueba: 410.33, Exactitud de prueba: 86.66\n",
      "Epoch 79, Perdida: 15.62, Exactitud: 98.01, Perdida de prueba: 462.18, Exactitud de prueba: 86.29\n",
      "Epoch 80, Perdida: 14.47, Exactitud: 98.10, Perdida de prueba: 434.34, Exactitud de prueba: 86.75\n",
      "Epoch 81, Perdida: 14.33, Exactitud: 98.09, Perdida de prueba: 469.63, Exactitud de prueba: 85.69\n",
      "Epoch 82, Perdida: 13.52, Exactitud: 98.17, Perdida de prueba: 403.25, Exactitud de prueba: 86.74\n",
      "Epoch 83, Perdida: 13.23, Exactitud: 98.23, Perdida de prueba: 450.32, Exactitud de prueba: 85.67\n",
      "Epoch 84, Perdida: 16.62, Exactitud: 97.97, Perdida de prueba: 391.83, Exactitud de prueba: 86.70\n",
      "Epoch 85, Perdida: 12.75, Exactitud: 98.34, Perdida de prueba: 425.79, Exactitud de prueba: 86.34\n",
      "Epoch 86, Perdida: 13.35, Exactitud: 98.14, Perdida de prueba: 454.86, Exactitud de prueba: 85.91\n",
      "Epoch 87, Perdida: 15.77, Exactitud: 98.12, Perdida de prueba: 407.35, Exactitud de prueba: 87.07\n",
      "Epoch 88, Perdida: 12.87, Exactitud: 98.25, Perdida de prueba: 410.29, Exactitud de prueba: 86.56\n",
      "Epoch 89, Perdida: 11.42, Exactitud: 98.46, Perdida de prueba: 408.86, Exactitud de prueba: 86.68\n",
      "Epoch 90, Perdida: 14.94, Exactitud: 98.23, Perdida de prueba: 404.17, Exactitud de prueba: 87.02\n",
      "Epoch 91, Perdida: 12.97, Exactitud: 98.31, Perdida de prueba: 412.97, Exactitud de prueba: 87.19\n",
      "Epoch 92, Perdida: 13.93, Exactitud: 98.24, Perdida de prueba: 405.48, Exactitud de prueba: 87.38\n",
      "Epoch 93, Perdida: 11.60, Exactitud: 98.44, Perdida de prueba: 401.46, Exactitud de prueba: 86.33\n",
      "Epoch 94, Perdida: 10.60, Exactitud: 98.49, Perdida de prueba: 426.85, Exactitud de prueba: 86.62\n",
      "Epoch 95, Perdida: 13.44, Exactitud: 98.30, Perdida de prueba: 405.42, Exactitud de prueba: 86.82\n",
      "Epoch 96, Perdida: 11.92, Exactitud: 98.33, Perdida de prueba: 443.23, Exactitud de prueba: 86.72\n",
      "Epoch 97, Perdida: 11.88, Exactitud: 98.37, Perdida de prueba: 422.66, Exactitud de prueba: 87.03\n",
      "Epoch 98, Perdida: 14.20, Exactitud: 98.24, Perdida de prueba: 413.64, Exactitud de prueba: 86.88\n",
      "Epoch 99, Perdida: 10.46, Exactitud: 98.58, Perdida de prueba: 432.53, Exactitud de prueba: 86.39\n",
      "Epoch 100, Perdida: 10.71, Exactitud: 98.57, Perdida de prueba: 409.89, Exactitud de prueba: 87.37\n"
     ]
    }
   ],
   "source": [
    "fitting(DNN,x_train,y_train_onehot,x_test,y_test_onehot,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "tarea_red_vanilla_FMNIST_TF2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
