{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4M5Lj0b9jNaG"
   },
   "source": [
    "## Red neuronal Perceptrón multicapa  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kMHueSlQkIeY"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eH3DG58ijNaH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_r9CetUjNaM"
   },
   "source": [
    "## Lectura del conjunto de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c_aVHNInjNaM",
    "outputId": "89a933fb-2a88-4e8f-9932-bbc0051e5faf"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "fashion_mnist = load_data()\n",
    "#read_data_sets(\"exercice/data\", one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGPrEVaNjNaS"
   },
   "source": [
    "## Separando el conjunto de datos en Entrenamiento y prueba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xISFxwdKjNaT"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)=fashion_mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HgclF9djNaW"
   },
   "source": [
    "Analizando el conjunto de Fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eg58cwGVjNaX",
    "outputId": "cdfebe5c-aa51-4501-9df1-c282f901021f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b7cbCu5yjNab"
   },
   "outputs": [],
   "source": [
    "imagendemo=x_train[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train = onehot_encoder.fit_transform(y_train.reshape([-1,1]))\n",
    "y_test = onehot_encoder.fit_transform(y_test.reshape([-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.reshape([-1,28*28]).astype('float')\n",
    "y_train=y_train.astype('float')\n",
    "x_test=x_test.reshape([-1,28*28]).astype('float')\n",
    "y_test=y_test.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0DIV1V6UjNae",
    "outputId": "7a52a89e-59fe-4176-c087-d343d33051e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b70bd4c408>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(imagendemo,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fn5aH2NdjNai"
   },
   "source": [
    "Las etiquetas numéricas pueden ser transformadas al nombre de la clase correspondiente usando el siguiente diccionario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "spo3e07vjNaj"
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    " 0: \"T-shirt/top\",\n",
    " 1: \"Trouser\",\n",
    " 2: \"Pullover\",\n",
    " 3: \"Dress\",\n",
    " 4: \"Coat\",\n",
    " 5: \"Sandal\",\n",
    " 6: \"Shirt\",\n",
    " 7: \"Sneaker\",\n",
    " 8: \"Bag\",\n",
    " 9: \"Ankle boot\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-1-fvDkajNam"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'T-shirt/top',\n",
       " 1: 'Trouser',\n",
       " 2: 'Pullover',\n",
       " 3: 'Dress',\n",
       " 4: 'Coat',\n",
       " 5: 'Sandal',\n",
       " 6: 'Shirt',\n",
       " 7: 'Sneaker',\n",
       " 8: 'Bag',\n",
       " 9: 'Ankle boot'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "b8irnPDhjNap"
   },
   "outputs": [],
   "source": [
    "def Neural_network_model(\n",
    "    n_nodes_hl1=500,\n",
    "    n_nodes_hl2=500,\n",
    "    n_nodes_hl3=500,\n",
    "    n_classes=10\n",
    "    ):\n",
    "    # Declarando las entradas y salidas\n",
    "    x=tf.placeholder('float',[None,784])\n",
    "    y=tf.placeholder('float')\n",
    "    \n",
    "    # Declarando las variables \n",
    "    \n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    \n",
    "    # Declarando la arquitectura\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(x,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
    "    \n",
    "    # Declarando la funcion de costo y optimizador\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=output\n",
    "                                                                   , labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    return dict(\n",
    "              x=x,\n",
    "              y=y,\n",
    "              output=output,\n",
    "              cost=cost,\n",
    "              optimizer=optimizer\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class batch_loader:\n",
    "    def __init__(self,data_x,data_y,batch_size=100):\n",
    "        self.x=data_x\n",
    "        self.y=data_y\n",
    "        self.index=0\n",
    "        self.len=len(data_x)\n",
    "        self.batch_size=batch_size\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index+self.batch_size<=self.len:\n",
    "            to_return=(self.x[self.index:self.index+self.batch_size,:],self.y[self.index:self.index+self.batch_size,:])\n",
    "            self.index+=self.batch_size\n",
    "            return to_return\n",
    "        elif self.index<self.batch_size:\n",
    "            to_return=(self.x[self.index:,:],self.y[self.index:,:])\n",
    "            self.index=self.len\n",
    "            return to_return\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(DNN, x_train,y_train, x_test,y_test, hm_epochs=10,batch_size=100):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for epoch_x, epoch_y in batch_loader(x_train,y_train):\n",
    "                feed_dict={DNN[\"x\"]: epoch_x, \n",
    "                           DNN[\"y\"]: epoch_y}\n",
    "                _, c, prediction,y   = sess.run([DNN[\"optimizer\"], DNN[\"cost\"]\n",
    "                                                 , DNN[\"output\"], DNN[\"y\"]], \n",
    "                                                feed_dict=feed_dict)\n",
    "                epoch_loss += c\n",
    "                \n",
    "            #Prueba con datos de entrenamiento\n",
    "            prediction,y   = sess.run([DNN[\"output\"], DNN[\"y\"]], feed_dict={DNN[\"x\"]:x_train, DNN[\"y\"]:y_train})\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            train_accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        \n",
    "            #Prueba con datos nunca antes vistos  \n",
    "            prediction,y   = sess.run([DNN[\"output\"], DNN[\"y\"]], feed_dict={DNN[\"x\"]:x_test, DNN[\"y\"]:y_test})\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "     \n",
    "            template = 'Epoch {:02}/{}, Perdida: {:.2f}, Exactitud entrenamiento: {:.2f}, Exactitud de prueba: {:.2f}'\n",
    "            print(template.format(epoch+1,hm_epochs,\n",
    "                             epoch_loss,\n",
    "                            train_accuracy.eval()*100,\n",
    "                            test_accuracy.eval()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-1bcfdec1eb28>:41: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch 01/100, Perdida: 574281410.70, Exactitud entrenamiento: 80.10, Exactitud de prueba: 78.52\n",
      "Epoch 02/100, Perdida: 206811110.70, Exactitud entrenamiento: 82.01, Exactitud de prueba: 79.13\n",
      "Epoch 03/100, Perdida: 141395304.81, Exactitud entrenamiento: 84.08, Exactitud de prueba: 80.23\n",
      "Epoch 04/100, Perdida: 106264510.88, Exactitud entrenamiento: 84.43, Exactitud de prueba: 80.34\n",
      "Epoch 05/100, Perdida: 83992133.61, Exactitud entrenamiento: 85.58, Exactitud de prueba: 81.28\n",
      "Epoch 06/100, Perdida: 69882070.14, Exactitud entrenamiento: 87.42, Exactitud de prueba: 82.21\n",
      "Epoch 07/100, Perdida: 56621287.18, Exactitud entrenamiento: 87.08, Exactitud de prueba: 81.43\n",
      "Epoch 08/100, Perdida: 48990442.58, Exactitud entrenamiento: 88.34, Exactitud de prueba: 82.42\n",
      "Epoch 09/100, Perdida: 41356337.48, Exactitud entrenamiento: 88.40, Exactitud de prueba: 82.40\n",
      "Epoch 10/100, Perdida: 37194443.47, Exactitud entrenamiento: 89.39, Exactitud de prueba: 82.88\n",
      "Epoch 11/100, Perdida: 33007778.44, Exactitud entrenamiento: 88.57, Exactitud de prueba: 81.62\n",
      "Epoch 12/100, Perdida: 30456616.98, Exactitud entrenamiento: 89.15, Exactitud de prueba: 82.01\n",
      "Epoch 13/100, Perdida: 25980127.74, Exactitud entrenamiento: 91.33, Exactitud de prueba: 83.63\n",
      "Epoch 14/100, Perdida: 23849082.12, Exactitud entrenamiento: 90.50, Exactitud de prueba: 83.49\n",
      "Epoch 15/100, Perdida: 21085010.03, Exactitud entrenamiento: 90.57, Exactitud de prueba: 83.04\n",
      "Epoch 16/100, Perdida: 19524744.36, Exactitud entrenamiento: 91.89, Exactitud de prueba: 83.17\n",
      "Epoch 17/100, Perdida: 16861721.52, Exactitud entrenamiento: 90.19, Exactitud de prueba: 82.37\n",
      "Epoch 18/100, Perdida: 15827537.10, Exactitud entrenamiento: 91.64, Exactitud de prueba: 83.14\n",
      "Epoch 19/100, Perdida: 16128954.50, Exactitud entrenamiento: 91.15, Exactitud de prueba: 83.01\n",
      "Epoch 20/100, Perdida: 14692707.87, Exactitud entrenamiento: 92.61, Exactitud de prueba: 84.21\n",
      "Epoch 21/100, Perdida: 13207222.62, Exactitud entrenamiento: 92.45, Exactitud de prueba: 83.56\n",
      "Epoch 22/100, Perdida: 12430902.07, Exactitud entrenamiento: 92.20, Exactitud de prueba: 83.58\n",
      "Epoch 23/100, Perdida: 11314515.19, Exactitud entrenamiento: 94.30, Exactitud de prueba: 84.39\n",
      "Epoch 24/100, Perdida: 10557250.59, Exactitud entrenamiento: 93.99, Exactitud de prueba: 84.55\n",
      "Epoch 25/100, Perdida: 11092242.21, Exactitud entrenamiento: 94.14, Exactitud de prueba: 84.62\n",
      "Epoch 26/100, Perdida: 10174681.43, Exactitud entrenamiento: 93.73, Exactitud de prueba: 84.44\n",
      "Epoch 27/100, Perdida: 9897754.55, Exactitud entrenamiento: 93.87, Exactitud de prueba: 84.49\n",
      "Epoch 28/100, Perdida: 9253737.46, Exactitud entrenamiento: 94.15, Exactitud de prueba: 84.85\n",
      "Epoch 29/100, Perdida: 8608080.38, Exactitud entrenamiento: 93.67, Exactitud de prueba: 84.42\n",
      "Epoch 30/100, Perdida: 8492414.53, Exactitud entrenamiento: 94.92, Exactitud de prueba: 85.29\n",
      "Epoch 31/100, Perdida: 8029856.57, Exactitud entrenamiento: 93.42, Exactitud de prueba: 84.36\n",
      "Epoch 32/100, Perdida: 7754982.87, Exactitud entrenamiento: 94.81, Exactitud de prueba: 84.88\n",
      "Epoch 33/100, Perdida: 7286987.51, Exactitud entrenamiento: 94.86, Exactitud de prueba: 84.46\n",
      "Epoch 34/100, Perdida: 7325933.84, Exactitud entrenamiento: 95.08, Exactitud de prueba: 84.78\n",
      "Epoch 35/100, Perdida: 6523871.64, Exactitud entrenamiento: 93.70, Exactitud de prueba: 84.49\n",
      "Epoch 36/100, Perdida: 6648995.67, Exactitud entrenamiento: 95.26, Exactitud de prueba: 85.25\n",
      "Epoch 37/100, Perdida: 6007379.78, Exactitud entrenamiento: 93.48, Exactitud de prueba: 84.36\n",
      "Epoch 38/100, Perdida: 5916974.37, Exactitud entrenamiento: 95.44, Exactitud de prueba: 85.06\n",
      "Epoch 39/100, Perdida: 5722245.01, Exactitud entrenamiento: 94.69, Exactitud de prueba: 84.91\n",
      "Epoch 40/100, Perdida: 5504028.68, Exactitud entrenamiento: 96.12, Exactitud de prueba: 85.12\n",
      "Epoch 41/100, Perdida: 5266415.53, Exactitud entrenamiento: 95.66, Exactitud de prueba: 85.15\n",
      "Epoch 42/100, Perdida: 5612347.44, Exactitud entrenamiento: 96.21, Exactitud de prueba: 85.16\n",
      "Epoch 43/100, Perdida: 4813445.02, Exactitud entrenamiento: 95.81, Exactitud de prueba: 85.07\n",
      "Epoch 44/100, Perdida: 4601103.09, Exactitud entrenamiento: 96.17, Exactitud de prueba: 85.34\n",
      "Epoch 45/100, Perdida: 4826096.87, Exactitud entrenamiento: 95.46, Exactitud de prueba: 84.82\n",
      "Epoch 46/100, Perdida: 4304593.94, Exactitud entrenamiento: 96.56, Exactitud de prueba: 85.32\n",
      "Epoch 47/100, Perdida: 4451558.88, Exactitud entrenamiento: 95.84, Exactitud de prueba: 85.45\n",
      "Epoch 48/100, Perdida: 4616116.26, Exactitud entrenamiento: 96.65, Exactitud de prueba: 85.51\n",
      "Epoch 49/100, Perdida: 4526306.87, Exactitud entrenamiento: 96.17, Exactitud de prueba: 85.31\n",
      "Epoch 50/100, Perdida: 4339608.75, Exactitud entrenamiento: 96.16, Exactitud de prueba: 85.14\n",
      "Epoch 51/100, Perdida: 3831144.65, Exactitud entrenamiento: 96.65, Exactitud de prueba: 85.60\n",
      "Epoch 52/100, Perdida: 3952618.95, Exactitud entrenamiento: 96.87, Exactitud de prueba: 85.52\n",
      "Epoch 53/100, Perdida: 3516070.11, Exactitud entrenamiento: 95.35, Exactitud de prueba: 84.92\n",
      "Epoch 54/100, Perdida: 3730798.69, Exactitud entrenamiento: 97.15, Exactitud de prueba: 86.15\n",
      "Epoch 55/100, Perdida: 3601696.37, Exactitud entrenamiento: 96.10, Exactitud de prueba: 85.02\n",
      "Epoch 56/100, Perdida: 3599577.25, Exactitud entrenamiento: 96.98, Exactitud de prueba: 85.97\n",
      "Epoch 57/100, Perdida: 3165457.38, Exactitud entrenamiento: 97.69, Exactitud de prueba: 86.03\n",
      "Epoch 58/100, Perdida: 2962685.96, Exactitud entrenamiento: 96.14, Exactitud de prueba: 85.27\n",
      "Epoch 59/100, Perdida: 3395631.32, Exactitud entrenamiento: 96.61, Exactitud de prueba: 85.49\n",
      "Epoch 60/100, Perdida: 3211129.20, Exactitud entrenamiento: 96.01, Exactitud de prueba: 85.28\n",
      "Epoch 61/100, Perdida: 3064019.99, Exactitud entrenamiento: 96.31, Exactitud de prueba: 85.15\n",
      "Epoch 62/100, Perdida: 3333927.87, Exactitud entrenamiento: 97.14, Exactitud de prueba: 85.54\n",
      "Epoch 63/100, Perdida: 2817619.97, Exactitud entrenamiento: 97.20, Exactitud de prueba: 85.99\n",
      "Epoch 64/100, Perdida: 3176893.99, Exactitud entrenamiento: 97.07, Exactitud de prueba: 85.63\n",
      "Epoch 65/100, Perdida: 2932749.37, Exactitud entrenamiento: 96.89, Exactitud de prueba: 85.55\n",
      "Epoch 66/100, Perdida: 2665412.28, Exactitud entrenamiento: 97.89, Exactitud de prueba: 86.15\n",
      "Epoch 67/100, Perdida: 2869177.23, Exactitud entrenamiento: 95.83, Exactitud de prueba: 85.11\n",
      "Epoch 68/100, Perdida: 2343472.53, Exactitud entrenamiento: 97.19, Exactitud de prueba: 85.91\n",
      "Epoch 69/100, Perdida: 2566598.50, Exactitud entrenamiento: 97.58, Exactitud de prueba: 85.87\n",
      "Epoch 70/100, Perdida: 2381128.59, Exactitud entrenamiento: 96.62, Exactitud de prueba: 85.27\n",
      "Epoch 71/100, Perdida: 3152159.71, Exactitud entrenamiento: 97.64, Exactitud de prueba: 86.15\n",
      "Epoch 72/100, Perdida: 2169423.79, Exactitud entrenamiento: 97.75, Exactitud de prueba: 86.38\n",
      "Epoch 73/100, Perdida: 2507131.07, Exactitud entrenamiento: 98.03, Exactitud de prueba: 86.57\n",
      "Epoch 74/100, Perdida: 2270496.27, Exactitud entrenamiento: 97.28, Exactitud de prueba: 86.09\n",
      "Epoch 75/100, Perdida: 2199215.22, Exactitud entrenamiento: 97.72, Exactitud de prueba: 86.01\n",
      "Epoch 76/100, Perdida: 2344315.72, Exactitud entrenamiento: 97.48, Exactitud de prueba: 86.05\n",
      "Epoch 77/100, Perdida: 2203942.00, Exactitud entrenamiento: 97.97, Exactitud de prueba: 86.47\n",
      "Epoch 78/100, Perdida: 2695730.16, Exactitud entrenamiento: 97.20, Exactitud de prueba: 86.27\n",
      "Epoch 79/100, Perdida: 2451871.44, Exactitud entrenamiento: 97.92, Exactitud de prueba: 85.99\n",
      "Epoch 80/100, Perdida: 2046963.51, Exactitud entrenamiento: 98.00, Exactitud de prueba: 86.23\n",
      "Epoch 81/100, Perdida: 2021781.07, Exactitud entrenamiento: 98.30, Exactitud de prueba: 86.32\n",
      "Epoch 82/100, Perdida: 2094264.50, Exactitud entrenamiento: 97.64, Exactitud de prueba: 86.22\n",
      "Epoch 83/100, Perdida: 1942852.63, Exactitud entrenamiento: 98.13, Exactitud de prueba: 86.50\n",
      "Epoch 84/100, Perdida: 2294757.65, Exactitud entrenamiento: 97.90, Exactitud de prueba: 85.90\n",
      "Epoch 85/100, Perdida: 2220751.96, Exactitud entrenamiento: 97.40, Exactitud de prueba: 86.26\n",
      "Epoch 86/100, Perdida: 1922981.35, Exactitud entrenamiento: 98.32, Exactitud de prueba: 86.41\n",
      "Epoch 87/100, Perdida: 2053418.20, Exactitud entrenamiento: 98.06, Exactitud de prueba: 86.91\n",
      "Epoch 88/100, Perdida: 1987472.07, Exactitud entrenamiento: 98.49, Exactitud de prueba: 86.68\n",
      "Epoch 89/100, Perdida: 1917369.74, Exactitud entrenamiento: 98.39, Exactitud de prueba: 86.75\n",
      "Epoch 90/100, Perdida: 1816594.95, Exactitud entrenamiento: 98.22, Exactitud de prueba: 86.54\n",
      "Epoch 91/100, Perdida: 1965052.34, Exactitud entrenamiento: 98.37, Exactitud de prueba: 86.50\n",
      "Epoch 92/100, Perdida: 1853210.63, Exactitud entrenamiento: 98.07, Exactitud de prueba: 86.48\n",
      "Epoch 93/100, Perdida: 1990647.51, Exactitud entrenamiento: 97.90, Exactitud de prueba: 86.44\n",
      "Epoch 94/100, Perdida: 1852350.85, Exactitud entrenamiento: 98.29, Exactitud de prueba: 86.23\n",
      "Epoch 95/100, Perdida: 1978194.61, Exactitud entrenamiento: 97.82, Exactitud de prueba: 86.39\n",
      "Epoch 96/100, Perdida: 1502246.14, Exactitud entrenamiento: 98.42, Exactitud de prueba: 86.84\n",
      "Epoch 97/100, Perdida: 1956679.29, Exactitud entrenamiento: 97.49, Exactitud de prueba: 86.15\n",
      "Epoch 98/100, Perdida: 1929382.27, Exactitud entrenamiento: 98.26, Exactitud de prueba: 86.57\n",
      "Epoch 99/100, Perdida: 1776681.65, Exactitud entrenamiento: 98.21, Exactitud de prueba: 86.65\n",
      "Epoch 100/100, Perdida: 1896027.10, Exactitud entrenamiento: 97.98, Exactitud de prueba: 86.59\n"
     ]
    }
   ],
   "source": [
    "DNN=Neural_network_model()\n",
    "train_neural_network(DNN,x_train,y_train,x_test,y_test,hm_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tarea de la red_Vanilla  TF1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
