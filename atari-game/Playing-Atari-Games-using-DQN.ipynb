{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jugando juegos de Atari usando DQN \n",
    "Atari 2600 es una consola de videojuegos popular de una compañia de juegos llamada Atari. Esta consola poseia muchos titulos populares como pong, space invaders, Ms Pacman, entre otros. En este ejercicio en particular se construye una red Q profunda (DQN por sus siglas en inglés) para jugar Ms Pacman.\n",
    "\n",
    "## Arquitectura de DQN\n",
    "En el ambiente de Atari, la imagen del juego en pantalla es el estado del ambiente. Por tanto, la imágen del juego es la que se introduce como entrada a la DQN y retorna el valor Q de todas las acciones en el estado. Ya que se están ocupando imágenes, en ves de unar una red neuronal profunda simple para aproximar el valor de Q, se ocupa una red convolucional ya que estas son muy efectivas para tratamiento de imágenes.\n",
    "\n",
    "Por tanto, la DQN es una red neuronal convolucional. Se alimenta la imágen del juego como una entrada en la red neuronal convolucional y da como salida el valor Q de todas las acciones en el estado.\n",
    "\n",
    "Como se muestra en la siguiente figura, las capas convolucionales extraen las caracteristicas de la imagen y producen un mapa de caracteristicas. Despues, se aplana este mapa de caracteristicas y se alimenta la red con este mapa. La parte de feedforward de la red toma este mapa aplanado como una entrada y retorna el valor Q de todas las acciones en el estado.\n",
    "\n",
    "\n",
    "![title](Images/4.png)\n",
    "\n",
    "Algo a considerar es que no hay operaciones de pooling. La operacion de pooling es util cuando se realizan tareas tales como deteccion de objetos, clasificacion de imágenes y otras cosas donde no se considera la posición del objeto en la imágen y solo se desea saber si el objeto deseado está presente en esta. \n",
    "\n",
    "Como lo que se desea es saber el estado del juego, la posición de los objetos es muy importante, por esto no se realiza una operación de pooling.\n",
    "\n",
    "\n",
    "## Implementando la red DQN\n",
    "\n",
    "Primero se importan las librerias necesarias. El autor realizó la prueba con tensorflow 2.0, en mi caso lo hice con la version 2.3.1 y usando la gpu de mi computadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, se crea el ambiente de Ms Pacman usando gym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MsPacman-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se establece el tamaño del estado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = (88, 80, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene el numero de acciones posibles por el agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de la imagen del juego\n",
    "\n",
    "Ya se mencionó que se alimentaria la red DQN con el estado del juego (imagen de la pantalla del juego), sin embarbo, alimentar la red directamente con la imagen cruda no es eficiente, ya que el tamaño de esta es de 210x160x3 lo cual es computacionalmente muy costoso.\n",
    "Para evitar esto, se hace un preprocesamiento a la imagen y despues se introduce a la red DQN. Primero, se corta y redimensiona el tamaño de la imágen, convirtiendo la imágen a escala de grises, normalizando y redimensionando la imágen a 88x80x1. \n",
    "\n",
    "Para esto se ocupa la siguiente funcion llamada preprocess_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = np.array([210, 164, 74]).mean()\n",
    "\n",
    "def preprocess_state(state):\n",
    "\n",
    "    #crop and resize the image\n",
    "    image = state[1:176:2, ::2]\n",
    "\n",
    "    #convert the image to greyscale\n",
    "    image = image.mean(axis=2)\n",
    "\n",
    "    #improve image contrast\n",
    "    image[image==color] = 0\n",
    "\n",
    "    #normalize the image\n",
    "    image = (image - 128) / 128 - 1\n",
    "    \n",
    "    #reshape the image\n",
    "    image = np.expand_dims(image.reshape(88, 80, 1), axis=0)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo la DQN \n",
    "\n",
    "Ahora, se construye la red Q profunda. Esta red recibe una imágen de entrada, la cual procesa con capas convolucionales y da como resultado los valores Q.\n",
    "\n",
    "Se definen tres capas convolucionales. Estas capas convolucionales extraen las caracteristicas de la imágen, mas adelante se aplanan los datos obtenidos y se introducen a dos capas densas, y la ultima capa retorna los valores Q.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        #define el tamaño del estado\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        #define el tamaño de la accion\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        #define el buffer de rejuego\n",
    "        self.replay_buffer = deque(maxlen=5000)\n",
    "        \n",
    "        #define el factor de descuento\n",
    "        self.gamma = 0.9  \n",
    "        \n",
    "        #define el valor epsilon\n",
    "        self.epsilon = 0.8   \n",
    "        \n",
    "        #define el ratio de actualizacion con el que se desea actualizar el objetivo de la red\n",
    "        self.update_rate = 1000    \n",
    "        \n",
    "        #define la red principal\n",
    "        self.main_network = self.build_network()\n",
    "        \n",
    "        #define el objetivo de la red\n",
    "        self.target_network = self.build_network()\n",
    "        \n",
    "        #copia los pesos de la red principal a la red de objetivo\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "\n",
    "    #Ahora se define una funcion llamada build_network la cual es necesaria para la DQN\n",
    "\n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=4, padding='same', input_shape=self.state_size))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (4, 4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "\n",
    "\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    #La red se entrena mediante pequeños bloques de muestras de trancisiones del buffer de rejuego.\n",
    "    #Por tanto, se define una función llamada store_transition la cual guarda la\n",
    "    # informacion de las trancisiones en el buffer de rejuego\n",
    "\n",
    "    def store_transistion(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "\n",
    "    #Para tener un control del balance exploración-explotacion, se seleccionan las acciones\n",
    "    #usando la política epsilon-greedy. Por tanto, se define esta funcion para seleccionar\n",
    "    #una accion usando la politica epsilon-greedy\n",
    "    \n",
    "    def epsilon_greedy(self, state):\n",
    "        if random.uniform(0,1) < self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        \n",
    "        Q_values = self.main_network.predict(state)\n",
    "        \n",
    "        return np.argmax(Q_values[0])\n",
    "\n",
    "    \n",
    "    #Entrenamiento de la red\n",
    "    def train(self, batch_size):\n",
    "        \n",
    "        #muestrea un minibloque de transiciones desde el buffer de replayr\n",
    "        minibatch = random.sample(self.replay_buffer, batch_size)\n",
    "        \n",
    "        #Calcula el valor de Q usando la red de objetivo\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if not done:\n",
    "                target_Q = (reward + self.gamma * np.amax(self.target_network.predict(next_state)))\n",
    "            else:\n",
    "                target_Q = reward\n",
    "                \n",
    "            #Calcula el valor de Q usando la red principal\n",
    "            Q_values = self.main_network.predict(state)\n",
    "            \n",
    "            Q_values[0][action] = target_Q\n",
    "            \n",
    "            #Entrena la red principal\n",
    "            self.main_network.fit(state, Q_values, epochs=1, verbose=0)\n",
    "            \n",
    "    #actualiza los pesos de la red de objetivo copiandolos desde la red principal\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando la red\n",
    "\n",
    "Ahora, se entrena la red, para esto, primero se establece el numero de episodios deseados.\n",
    "en el codigo original realizan más de 10 episodios, si mal no recuerdo eran 50 o 100, pero son muy tardados por lo que yo solo ejecuté 10 episodios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define el numero de pasos de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define el tamaño del bloque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se establece el numero de imágenes de la pantalla despues del juego que se desean considerar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_screens = 4     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se instancia la clase DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0 ,Return 290.0\n",
      "Episode:  1 ,Return 140.0\n",
      "Episode:  2 ,Return 440.0\n",
      "Episode:  3 ,Return 270.0\n",
      "Episode:  4 ,Return 200.0\n",
      "Episode:  5 ,Return 280.0\n",
      "Episode:  6 ,Return 450.0\n",
      "Episode:  7 ,Return 330.0\n",
      "Episode:  8 ,Return 300.0\n",
      "Episode:  9 ,Return 210.0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "time_step = 0\n",
    "\n",
    "#for para cada episodio\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    #establece el valor de retorno en 0\n",
    "    Return = 0\n",
    "    \n",
    "    #realiza el preprocesamiento de la imagen en pantalla, además de reiniciar el entorno\n",
    "    state = preprocess_state(env.reset())\n",
    "\n",
    "    #for que recorre cada paso de tiempo del episodio\n",
    "    for t in range(num_timesteps):\n",
    "        \n",
    "        #renderiza el entorno\n",
    "        env.render()\n",
    "        \n",
    "        #actualiza el paso de tiempo\n",
    "        time_step += 1\n",
    "        \n",
    "        #actualiza la red de objetivo\n",
    "        if time_step % dqn.update_rate == 0:\n",
    "            dqn.update_target_network()\n",
    "        \n",
    "        #seleciona la accion\n",
    "        action = dqn.epsilon_greedy(state)\n",
    "        \n",
    "        #realiza la accion seleccionada\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        #preprocesa el siguiente estado\n",
    "        next_state = preprocess_state(next_state)\n",
    "        \n",
    "        #guarda la información de transición\n",
    "        dqn.store_transistion(state, action, reward, next_state, done)\n",
    "        \n",
    "        #actualiza el estado actual a el estado siguiente\n",
    "        state = next_state\n",
    "        \n",
    "        #actualiza el valor de retorno añadiendo el valor de la recompenzza\n",
    "        Return += reward\n",
    "        \n",
    "        #si el episodio ha terminado, muestra el valor de retorno y sale del for interno\n",
    "        if done:\n",
    "            print('Episode: ',i, ',' 'Return', Return)\n",
    "            break\n",
    "            \n",
    "        #si el numero de transiciones del buffer de rejuego es mayor\n",
    "        #que el tamaño del bloque, entonces se entrena la red\n",
    "        if len(dqn.replay_buffer) > batch_size:\n",
    "            dqn.train(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n",
    "Antes de correr este código, ya habia probado otro ejemplo usando DQN tambien utilizando la libreria de gym, en ese caso fue un péndulo invertido y el numero de acciones y valores del ambiente es menor por lo que era mas sencillo de comprender. Este ejemplo por otra parte es mas enriquecedor ya que trabaja a partir de una imagen de entrada, lo cual se acerca más a un ambiente real en el que un agente mediante vision artificial pueda observar su ambiente y realizar acciones\n",
    "Obviamente a mayor complejidad con la entrada, el tiempo de entrenamiento tambien aumenta, estos 10 episodios le llevó aproximadamente una hora a mi computadora usando la GPU, y lo recomendado era hacer más de 50 episodios para comenzar a tener un agente \"inteligente\". Aunque no se necesita una base de datos para el entrenamiento, el aprendizaje aun lleva mucho tiempo por las \"simulaciones\" que realiza el agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
